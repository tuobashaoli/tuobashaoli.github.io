---
categories: blog
date: '2024-02-23 22:29:18'
description: 一些深度学习的概念
layout: post
published: False
title: "关于深度学习"
tags: deeplearning
---

# 如何进行梯度下降

以拟合一元一次函数的损失函数为例，如下

![avatar](/assets/images/dl1.PNG)

其中的m，就是用于计算梯度的训练数据的数量，当一次迭代中把全部的训练数据都投入来计算梯度，即m  = len(train_data)，自然得到的梯度向量就更加准确，但是这个时候所消耗的计算资源就十分高，这就是**批量梯度下降**

既然这个对计算资源要求高，那么为了降低计算资源，每次迭代计算梯度时，就不用全部的训练数据了，就随机选取一个样本，即 m = 1,
这样将计算资源的消耗降至最低，代价就是得到的梯度不准确，梯度的向量在每次迭代中差距可能较大，这样的结果可能会来回震荡，甚至不会收敛，这就是**随机梯度下降**，我愿称之为**猴子梯度下降（其实不是）**

那么这时，很容易从以上俩者做个折中，即每次随机选择小批量的训练数据，来计算梯度，即 1<m<<len(train_data),这就是**小批量梯度下降**,这样，提高了模型参数变化的收敛性，也不至于让迭代速度变慢，同时，由于训练数据选择是随机的，这也降低了陷入局部最优解的概率。

总结：

![avatar](/assets/images/dl_gradient.PNG)

学习视频：[什么是小批量梯度下降，和批量梯度下降、随机梯度下降有什么不同](https://www.bilibili.com/video/BV13p4y1g7eQ/?share_source=copy_web&vd_source=1e1a8470626ff3354bcd0d9c492ad7d2)